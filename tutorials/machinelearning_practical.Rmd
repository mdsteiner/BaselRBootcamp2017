---
title: "Practical: Machine Learning"
author: "BaselRBootcamp 2017"
output: html_document
---

```{r, echo = FALSE, fig.align = 'center', out.width = "50%", fig.cap = "Source: https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer"}
knitr::include_graphics("https://uploads.toptal.io/blog/image/443/toptal-blog-image-1407508081138.png")
```


```{r, echo = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, fig.align = 'center')
```


### Slides

Here a link to the lecture slides for this session: LINK

### Overview

In this practical you'll conduct machine learning analyses on a dataset on heart disease. You will see how well many different machine learning models can predict new data. By the end of this practical you will know how to:

1. Create separate training and test data
2. Fit a model to data
3. Make predictions from a model
4. Compare models in how well they can predict new data.

### Glossary and packages

Here are the main functions and packages you'll be using. For more information about the specific models, click on the link in *Additional Details*.

| Algorithm| Function| Package | Additional Details |
|:------|:--------|:----|:----|
|     Regression|    `glm()`| Base R| https://bookdown.org/ndphillips/YaRrr/regression.html#the-linear-model|
|     Support Vector Machines|    `svm()`| `e1071` |https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html |
|     Decision Trees|    `rpart()`| `rpart` | https://statweb.stanford.edu/~lpekelis/talks/13_datafest_cart_talk.pdf|
| Random Forests | `randomForest()` | `randomForest` | http://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/|


```{r, echo = TRUE, eval = FALSE}
# Install all packages necessary for the practical
#  Only necessary if you don't already have them

install.packages(
  c("e1071", 
    "rpart", 
    "randomForest", 
    "tidyverse", 
    "yarrr", 
    "FFTrees"))
```


### Examples

- The following examples will take you through all steps of the machine learning process, from creating training and test data, to fitting models, to making predictions. Follow along and try to see how piece of code works!

```{r, eval = FALSE, echo = TRUE}
# -----------------------------------------------
# A step-by-step tutorial for conducting machine learning
# ------------------------------------------------

# -----------------------
# Part A:
# Load libraries
# -----------------------

library(e1071)          # for svm()
library(randomForest)   # for randomForest()
library(rpart)          # for rpart()
library(yarrr)          # for pirateplot()
library(tidyverse)      # for datawrangling and ggplot2
library(FFTrees)        # for the heartdisease data

# -----------------------
# Part B:
# Create heart a dataset containing all data
# -----------------------

heart <- heartdisease   # Save a copy of the heartdisease data as heart

for(i in 1:ncol(heart)) {  # Convert character columns and diagnosis to factor
  
  if(class(heart[,i]) == "character" | names(heart)[i] == "diagnosis") {
    
    heart[,i] <- factor(heart[,i])}
  
}

# -----------------------
# Part C: Create separate training and test data 
#  heart_train, heart_test
# -----------------------

set.seed(100)   # To fix the randomization
train_samples <- sample(nrow(heart), size = 125)
test_samples <- setdiff(1:nrow(heart), train_samples)

heart_train <- heart[train_samples, ]
heart_test <- heart[test_samples, ]

# ------------------------------
# Part I: Create models from training data
# ------------------------------

glm_model <- glm(formula = chol ~ ., data = heart_train)
rpart_model <- rpart(formula = chol ~ ., data = heart_train)
randomforest_model <- randomForest(formula = chol ~ ., data = as.data.frame(heart_train))
svm_model <- svm(formula = chol ~ ., data = heart_train)

# ------------------------------
# Part II: Calculate fits for each model for heart_train
#  predict(model, heart_train)
# ------------------------------

glm_fit <- predict(glm_model, heart_train)
rpart_fit <- predict(rpart_model, heart_train)
randomforest_fit <- predict(randomforest_model, heart_train)
svm_fit <- predict(svm_model, heart_train)

# Calculate fitting absolute errors and put in a dataframe

fit_mae_df <- data_frame(index = 1:nrow(heart_train),
                         glm = abs(glm_fit - heart_train$chol),
                         rpart = abs(rpart_fit - heart_train$chol),
                         randomforest = abs(randomforest_fit - heart_train$chol),
                         svm = abs(svm_fit - heart_train$chol))
)

# ------------------------------
# Part III: Plot fitting results
# ------------------------------

# Create a second reshaped dataframe for ggplot

fit_mae_df2 <- fit_mae_df %>% 
  gather(model, mae, -index)

# Create violin plot with ggplot

ggplot(fit_mae_df2, 
       aes(model, mae)) + 
       geom_boxplot(aes(fill = factor(model))) +
       labs(title = "Cholestorol Fitting Performance", 
            y = "Mean Absolute Error (MAE)")

# Or a pirateplot (using the yarrr package)
pirateplot(data = as.data.frame(fit_mae_df[,2:5]), 
           main = "Cholesterol Fitting Accuracy")

# ------------------------------
# Part IV: Calculate predictions for each model for heart_test
# predict(model, heart_test)
# ------------------------------

# Calculate predictions for each model for heart_test

glm_pred <- predict(glm_model, heart_test)
rpart_pred <- predict(rpart_model, heart_test)
randomforest_pred <- predict(randomforest_model, heart_test)
svm_pred <- predict(svm_model, heart_test)

# Calculate prediction ae (mean absolute error)

truth_test <- heart_test$chol

glm_pred_ae <- abs(glm_pred - truth_test)
rpart_pred_ae <- abs(rpart_pred - truth_test)
randomforest_pred_ae <- abs(randomforest_pred - truth_test)
svm_pred_ae <- abs(svm_pred - truth_test)


pred_mae_df <- data_frame(index = 1:nrow(heart_test),
                          glm = glm_pred_ae,
                          rpart = rpart_pred_ae,
                          randomforest = randomforest_pred_ae,
                          svm = svm_pred_ae)

# ------------------------------
# Part V: Plot prediction results
# ------------------------------

# Create a second reshaped dataframe for ggplot
pred_mae_df2 <- pred_mae_df %>% 
              gather(model, mae, -index)

# Create violin plot

ggplot(pred_mae_df2, 
       aes(model, mae)) + 
       geom_boxplot(aes(fill = factor(model))) +
       labs(title = "Cholestorol Prediction Performance", 
            y = "Mean Absolute Error (MAE)")

# Or a pirateplot
pirateplot(data = as.data.frame(pred_mae_df[,2:5]), 
           main = "Cholesterol Prediction Accuracy")

```


## Tasks

- Note, most of this practical will be copying and pasting code from the Examples and only making small changes.
- You should start by copying and pasting all of the code in the examples into a new .R file.
- Try running pieces of the code line by line and understand what it's doing.

#### Part A

A. Run the code in part A to load all of the necessary pakages. For this practical, we'll use the `heartdisease` dataframe from the `FFTrees` package, we'll also use the `e1071`, `randomForest`, `rpart`, `ggplot2`, and `yarrr` pacakges. 

```{r, message = FALSE, echo = FALSE, eval = TRUE, warning = FALSE}
# Load libraries

library(tidyverse)      # for datawrangling and ggplot2
library(e1071)          # for svm()
library(randomForest)   # for randomForest()
library(rpart)          # for rpart()
library(yarrr)          # for pirateplot()
library(FFTrees)        # for the heartdisease data
```

#### Part B: Create heart

B. Now run the code in Part B to save a copy of the data as a tibble called `heart`.

```{r, echo = FALSE, eval = TRUE}
# -----------------------
# Part B:
# Create heart
# -----------------------

# Create heart a dataset containing all data
heart <- heartdisease

# Some of the columns need to be converted to factors
for(i in 1:ncol(heart)) {
  
  if(class(heart[,i]) == "character" | names(heart[i]) == "diagnosis") {
    
   heart[,i] <- factor(heart[,i])}
  
}

```


C. Take a look at the data by printing the first few rows with `head()`. It should look like this:

```{r, echo = FALSE, eval = TRUE}
head(heart)
```

#### Part C: Create training and test data

1. Run the code in Part C to create separate dataframes `heart_train` for model training and `heart_test`. Print each of these dataframes to make sure they look ok.

```{r, echo = FALSE}

# -----------------------
# Part C: Create training and test data 
#  heart_train, heart_test
# -----------------------

# Determine which samples are for training and which are for testing

set.seed(100)   # To fix the randomization
train_samples <- sample(nrow(heart), size = 125)
test_samples <- setdiff(1:nrow(heart), train_samples)

# Create training and test data
heart_train <- heart[train_samples, ]
heart_test <- heart[test_samples, ]
```


#### Part I: Train models on  `trestbps`

2. In our analyses, we will try to predict each patient's value of `trestbps` -- their resting blood pressure, as a function of all other variables in the data. Following Part I, create four new objects `glm_trestbps_train`, `svm_trestbps_train`, `rpart_trestbps_train`, and `randomforest_trestbps_train` predicting `trestbps`.


```{r, echo = FALSE}
# ------------------------------
# Part I: Create models from training data
# ------------------------------

glm_model <- glm(formula = trestbps ~ ., data = heart_train)
svm_model <- svm(formula = trestbps ~ ., data = heart_train)
rpart_model <- rpart(formula = trestbps ~ ., data = heart_train)
randomforest_model <- randomForest(formula = trestbps ~ ., data = heart_train)
```

#### Part II: Calculate fits for training data

3. Following Part II, calculate fits for each model with `predict(model, heart_train)`, then create `fit_mae_df` containing the absolute fitting errors for each model. The code will be almost identical to what is in the Example. All you need to do is change the value of `truth_train` to the correct column in `heart_train`.

```{r, echo = FALSE}
# ------------------------------
# Part II: Calculate fits for each model for heart_train
#  predict(model, heart_train)
# ------------------------------

glm_fit <- predict(glm_model, heart_train)
svm_fit <- predict(svm_model, heart_train)
rpart_fit <- predict(rpart_model, heart_train)
randomforest_fit <- predict(randomforest_model, heart_train)

# Calculate fitting absolute errors and put in a dataframe

truth_train <- heart_train$trestbps

fit_mae_df <- data_frame(index = 1:nrow(heart_train),
                         glm = abs(glm_fit - truth_train),
                         svm = abs(svm_fit - truth_train),
                         rpart = abs(rpart_fit - truth_train),
                         randomforest = abs(randomforest_fit - truth_train))
```


#### Part III: Plot fitting results

4. Following Part III, restructure the data into a format ready for plotting, and then plot the training results. Don't forget to change the labels to reflect that we are predicting trestbps, not cholesterol!


```{r, eval = FALSE, echo = FALSE}
# ------------------------------
# Part III: Plot Training results
# ------------------------------

# Create a second reshaped dataframe for ggplot

fit_mae_df2 <- fit_mae_df %>% 
  gather(model, mae, -index)

# Create violin plot with ggplot

ggplot(fit_mae_df2, 
       aes(model, mae)) + 
       geom_boxplot(aes(fill = factor(model))) +
       labs(title = "Trestbps Fitting Performance", 
            y = "Mean Absolute Error (MAE)")

# Or a pirateplot with yarrr
yarrr::pirateplot(data = as.data.frame(fit_mae_df[,2:5]), 
                  main = "Trestbps Fitting Accuracy, ")
```

#### Part IV: Calculate predictions for test data

5. Following Part IV, calculate predictions for each model based on `heart_test`, and then calculate the prediction error as the difference between the truth and the model predictions. Don't forget to change the value of `truth_test` to reflect the true value for the current analysis!

```{r, eval = FALSE, echo = FALSE}
# ------------------------------
# Part IV: Calculate predictions for each model for heart_test
# predict(model, heart_test)
# ------------------------------

# Calculate predictions for each model for heart_test

glm_pred <- predict(glm_model, heart_test)
svm_pred <- predict(svm_model, heart_test)
rpart_pred <- predict(rpart_model, heart_test)
randomforest_pred <- predict(randomforest_model, heart_test)

# Calculate prediction ae (mean absolute error)

truth_test <- heart_test$trestbps

glm_pred_ae <- abs(glm_pred - truth_test)
svm_pred_ae <- abs(svm_pred - truth_test)
rpart_pred_ae <- abs(rpart_pred - truth_test)
randomforest_pred_ae <- abs(randomforest_pred - truth_test)

pred_mae_df <- data_frame(index = 1:nrow(heart_test),
                          glm = glm_pred_ae,
                          svm = svm_pred_ae,
                          rpart = rpart_pred_ae,
                          randomforest = randomforest_pred_ae)
```

#### Part V: Plot prediction results

6. Following Part V, plot the prediction results! Which model was the best in predicting resting heart rate in the test set? Don't forget to change the names of the labels to reflect that we're predicting trestbps!

```{r, eval = FALSE, echo = FALSE}
# ------------------------------
# Part V: Plot prediction results
# ------------------------------

# Create a second reshaped dataframe for ggplot
pred_mae_df2 <- pred_mae_df %>% 
              gather(model, mae, -index)

# Create violin plot

ggplot(pred_mae_df2, 
       aes(model, mae)) + 
       geom_boxplot(aes(fill = factor(model))) +
       labs(title = "Resting blood pressure Prediction Performance", 
            y = "Mean Absolute Error (MAE)")


# Or a pirateplot
yarrr::pirateplot(data = as.data.frame(pred_mae_df[,2:5]), 
                  main = "Resting blood pressure Prediction Accuracy")
```

## Extras and Challenges

7. We have fitted many models and made predictions, but we haven't seen much about what is in them and what they contain. Try printing each of the model objects and applying the `summary()` function to them to see what information they print. For example, run `glm_model`, and `summary(glm_model)` to look at the glm model (do the same for the other models). Do these outputs make sense? Try looking at the help menu for each model (e.g.; `?randomForest`) to see if you can find any help! 

8. Of all the models we've used, the only one you can easily visualize is a decision tree created from `rpart`. Try visualizing the trees you created by running `plot(rpart_model)` and then `text(rpart_model)`. What does the rpart model tell you about the data?

9. The randomForest model is difficult to visualize, but you can see which variables are deemed to be important in the dataset by accessing the result `importance()`. Try visualising the importance of the variables by running the following code. What do the results tell you? Do you see connections between the randomForest model and the regression (`glm()`) and decision tree (`rpart()`) models?

```{r, echo = TRUE, eval = FALSE}
# Create a dataframe of the importance results
importance_results <- as.data.frame(importance(randomforest_model))
importance_results$feature <- rownames(importance_results)

# Plot the result as a barplot
ggplot(importance_results, aes(feature, IncNodePurity, fill = feature)) + 
  geom_col()
```

10. You'll notice in Part C that we trained the models on 125 cases (out of the 303) in the full dataset. In other words, we trained the data on about half of the total cases. Try repeating the same machine learning process as above (for either cholesterol or resting heart rate), but instead of training the models on 125 cases, try training it on only 50 cases (about 15% of the data). How do you think having fewer cases in the training data will affect accuracy in fitting and prediction? When you are done, try training the models based on 250 cases (over 80% of the data) and then making predictions on the remaining cases. 
   
11. In all of our machine learning, we have allowed all models to use all data in the `heartdisease` dataset. What do you think would happen if we only let the models use a single predictor like `age`? Test your prediction by replicating the machine learning process, but *only* allow the models to make predictions based on `age`. Does one model substantially outperform the others? What happens if you include three variables such as `age`, `cp` and `slope`?
   
12. So far we have only predicted continuous variables (`chol` and `trestbps`). However, all the machine learning models we have covered can also predict categorical values -- like whether patient's have a diagnosis value of 0 or 1. Try replicating your previous analyses, but now predict `diagnosis`. Here are some hints to help you:

 - In order to tell R that diagnosis is a factor (that is, a categorical variable), you need to convert it to a factor when running `formula()` each time you fit a model. For example, to run the support vector machines model, you'd run `svm_model <- svm(formula = factor(diagnosis) ~., data = heart_train)`. For the `glm()` model, you will additionally need to include the argument `family = 'binomial'` to tell the algorithm to run a logistic regression. See the help menu for `?glm()` for examples.

 - You will need to change the fitting and prediction accuracy code in parts II and IV. Specifically, you can measure model error as the *disagreement* between model predictions and the true values. 
   
```{r, echo = TRUE, eval = FALSE}
# When modelling categorical values, then calculate model DISAGREEMENT 
#  as the percentage of cases where the model predictions 
#  are different from the true values

glm_pred_d <- mean(glm_pred != truth_test)     # Percent of cases where glm model is incorrect
svm_pred_d <- mean(svm_pred != truth_test)     # Percent of cases where svm model is incorrect 
rpart_pred_d <- mean(rpart_pred != truth_test) # [...]
randomforest_pred_d <- mean(randomforest_pred != truth_test) # [...]

# Combine model prediction errors just like in the Example

pred_d_df <- data_frame(index = 1:nrow(heart_test),
                          glm = glm_pred_d,
                          svm = svm_pred_d,
                          rpart = rpart_pred_d,
                          randomforest = randomforest_pred_d)
```

13. So far we've only looked at the `heartdisease` data. Try conducting a similar analysis on the `ACTG175` data from the `speff2trial` package. For example, you could try to predict the number of days until each patient experiences a major negative event (`days`). Which of the different machine learning algorithms performs the best in predicting new data from this dataset? Do you discover any challenges in working with dataset that weren't present in the `heartdisease` data?


# Additional reading

- For more advanced machine learning functionality in R, check out the `caret` package [caret documentation link](http://topepo.github.io/caret/index.html) and the `mlr` package [mlr documentation link](https://cran.r-project.org/web/packages/mlr/vignettes/mlr.html). Both of these packages contain functions that automate most aspects of the machine learning process.

- To read more about the fundamentals of machine learning and statistical prediction, check out [An Introduction to Statistical Learning by James et al.](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370)
