---
title: "Machine Learning"
subtitle: ""
author: "The R Bootcamp<br/>Twitter: <a href='https://twitter.com/therbootcamp'>@therbootcamp</a>"
date: "September 2017"
output:
  xaringan::moon_reader:
    css: ["default", "my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)
```


# What is machine learning?

- Algorithms autonomously learning from data.
- Given data, an algorithm tunes its parameters to match the data, understand how it works, and make predictions for what will occur in the future.


---
# What is the history of machine learning?

- 1805 - 1809: Legendre and Gauss discover least squares. Soon after Galton defines **Regression** in a biological context, followed by Pearson for purely statistical analyses.

- 1952: Arthur Samuel creates first computer learning program for learning checkers and coins the term **Machine Learning** in 1959. 

- 1957: Frank Rosenblatt creates first **Neural Network** to simulate the thought process of the human brain.

- 1963: First algorithm for **Support Vector Machines** is developed by Vapnik & Chervonenkis.

- 1967: **Nearest neighbor algorithm** is developed for classification

- 1984: Breiman & Olshen publish the CART algorithm for **Decision Trees**, followed by Quinlan who publishes the ID3 algorithm followed by C4.5

- 1986: Rina Dechter introduces **Deep Learning**, with many subsequent updates in the 2000s.

- 1995: Tin Kam Ho develops first algorithm for **Random Forests**

Sources: Wikipedia, Bernard Marr, "A Short History of Machine Learning", Forbes.


---
# Who uses machine learning?

Everyone!

```{r, echo = FALSE, out.width = "30%"}
knitr::include_graphics("images/googlelogo.png")
```


```{r, echo = FALSE, out.width = "10%"}
knitr::include_graphics("images/facebooklogo.png")
```


Google, Apple, Facebook, Amazon


> Machine learning drives our algorithms for demand forecasting, product search ranking, product and deals recommendations, merchandising placements, fraud detection, translations, and much more. ~ Jeff Bezos

---
# What is the basic machine learning process?

```{r, echo = FALSE, out.width = "95%"}
knitr::include_graphics("images/MLdiagram.png")
```


---
# Why do we separate training from prediction?

- Data comes from two processes: *Signal* and *Noise* (aka Error).
    


```{r, echo = FALSE, fig.width = 7, fig.height = 3, dpi=200, out.width = "80%", fig.align = 'center'}

set.seed(100)
x <- seq(0, 5, length.out = 50)
noise <- rnorm(50, mean = 0, sd = 2.5)

model_fun <- function(x) {x ^ 3 - 4 * x ^ 2 + .5 * x + 5}

y <- model_fun(x)
y_obs <- model_fun(x) + noise

par(mar = c(3, 4, 3, 1))

par(mfrow = c(1, 3))

plot(x, y_obs, main = "Data", xlab = "", ylab = "", col = "black")

# segments(x, y, x, y_obs)

# lines(y, y = y_obs)



# Plot 1

plot(x, y_obs, main = "Signal", xlab = "", ylab = "", col = "lightgray")
curve(model_fun, from = 0, to = 5, add = TRUE, col = "green", lwd = 2)

# segments(x, y, x, y_obs)

# lines(y, y = y_obs)



# Plot 2

plot(x, y_obs, main = "Noise", xlab = "", ylab = "")
 curve(model_fun, from = 0, to = 5, add = TRUE, col = "darkgray", lwd = .5)

 segments(x, y, x, y_obs)

# lines(y, y = y_obs)


# 
# # Plot 3
# 
# plot(x, y_obs, main = "A bad model tries to fit everything", xlab = "", ylab = "")
# curve(model_fun, from = 0, to = 5, add = TRUE, col = "darkgray", lwd = .5)
# 
# text(.5, 20, "Hey I can draw a line through all points\nI don't have any error!", adj = 0)
# 
# 
# lines(x, y_obs)
# 
# # Plot 4
# 
# plot(x, y_obs, main = "A good model will try to focus on the signal", xlab = "", ylab = "")
# curve(model_fun, from = 0, to = 5, add = TRUE, col = "blue", lwd = 2)
# 
# text(.5, 20, "I won't try to fit all points because\nI think there is random error", adj = 0)


# lines(x, y_obs)


```




---
# Why do we separate training from prediction?

- A good model will sacrifice accuracy in fitting data, to focus on signal
    

```{r, echo = FALSE, fig.width = 7, fig.height = 3, dpi=200, out.width = "80%", fig.align = 'center'}

set.seed(100)
x <- seq(0, 5, length.out = 50)
noise <- rnorm(50, mean = 0, sd = 2.5)

model_fun <- function(x) {x ^ 3 - 4 * x ^ 2 + .5 * x + 5}

y <- model_fun(x)
y_obs <- model_fun(x) + noise

par(mar = c(3, 4, 3, 1))

par(mfrow = c(1, 3))

plot(x, y_obs, main = "Data", xlab = "", ylab = "", col = "black")

# segments(x, y, x, y_obs)

# lines(y, y = y_obs)



# Plot 1

plot(x, y_obs, main = "Good Model", xlab = "", ylab = "", col = "darkgray")

lines(x, y, col = "blue")

# curve(model_fun, from = 0, to = 5, add = TRUE, col = "green", lwd = 2)

text(.5, 20, "Fitting error = Medium", adj = 0)
text(.5, 15, "Prediction error = Low", adj = 0)


# segments(x, y, x, y_obs)

# lines(y, y = y_obs)



# Plot 2

plot(x, y_obs, main = "Bad Model", xlab = "", ylab = "")

text(.5, 20, "Fitting error = None", adj = 0)
text(.5, 15, "Prediction error = High", adj = 0)


lines(x, y_obs, col = "red")
# lines(y, y = y_obs)


# 
# # Plot 3
# 
# plot(x, y_obs, main = "A bad model tries to fit everything", xlab = "", ylab = "")
# curve(model_fun, from = 0, to = 5, add = TRUE, col = "darkgray", lwd = .5)
# 
# text(.5, 20, "Hey I can draw a line through all points\nI don't have any error!", adj = 0)
# 
# 
# lines(x, y_obs)
# 
# # Plot 4
# 
# plot(x, y_obs, main = "A good model will try to focus on the signal", xlab = "", ylab = "")
# curve(model_fun, from = 0, to = 5, add = TRUE, col = "blue", lwd = 2)
# 
# text(.5, 20, "I won't try to fit all points because\nI think there is random error", adj = 0)


# lines(x, y_obs)


```



---
# Why do we separate training from prediction?

.pull-left3[

Just because an algorithm can fit data (in training) well, does *not* necessarily mean that it will *predict* new data well.
    
    
    
    
Because we care about predicting new data, we always need to test how well a model can predict new data *after* it has been trained on old data.

]

.pull-right3[


> "Prediction is difficult, especially when it is about the future"

```{r, fig.cap = "Niels Bohr, Nobel Laureate in Physics", echo = FALSE, out.width = "40%", fig.align = 'center'}
knitr::include_graphics("images/bohr.jpg")
```

]



---
# What machine learning algorithms are there?

.pull-left3[

- There are hundreds if not thousands of machine learning algorithms from many different fields.
    - E.g.; Computer vision, Natural language processing, reinforcement learning, graphical models


- In this section, we will focus on 4:

| Algorithm|
|:------|
|     Regression| 
|     Decision Trees|   
|     Random Forests|   
|     Support Vector Machines|   

]

.pull-right3[

Wikipedia lists 57 *Categories* of machine learning algorithms, each with dozens of examples

```{r, echo = FALSE, eval = TRUE}
knitr::include_graphics("images/wikipediaml.png")
```

]



---
# Regression

.pull-left3[

In regression, the criterion is modeled as the weighted sum of predictors times *weights* $\beta_{1}$, $\beta_{2}$.

### Example: Default on a loan

Imagine a dataset predicting whether someone defaults on their loan based on characteristics such as age and income.

One could model the default risk as:

$$Risk = Age \times \beta_{age} + Income \times \beta_{income} + ...$$

Training a model means finding values of $\beta_{Age}$ and $\beta_{Income}$ that 'best' match the training data.

]


.pull-right3[

Create regressions using the `glm()` function (part of base-R)

```{r, eval = FALSE}
# Format of a regression model
glm(formula, data)

# Loan Example
loan_glm_model <- glm(formula = risk ~ ., 
                      data = loan_data)

```


```{r, echo = FALSE, eval = TRUE, out.width = "50%", fig.align = 'center'}
knitr::include_graphics("images/regression.png")
```

]


---
# Decision Trees

.pull-left3[

In decision trees, the criterion is modeled as a sequence of logical Yes or No questions.

### Example: Default on a loan

```{r, echo = FALSE, eval = TRUE}
knitr::include_graphics("images/defaulttree.png")
```


]

.pull-right3[

Create decision trees using the `rpart` package

```{r, eval = FALSE}
# Load the rpart package
library(rpart)

# Calculating a decision tree in R
rpart(formula, data)

# Loan Example
loan_rpart_model <- rpart(formula = risk ~ ., 
                          data = loan_data,
                          method = "anova")
```


]

---
# Advanced algorithms

.pull-left3[

### Support Vector Machines


```{r, echo = FALSE, eval = TRUE, out.width = "50%", fig.align = 'center'}
knitr::include_graphics("images/supportvectormachine.png")
```

```{r, eval = FALSE}
# Creating support vector machine model
library(e1071)

svm_model <- svm(formula = risk ~ .,
                 data = loan_data)
```



]


.pull-right3[

### Random Forests


```{r, echo = FALSE, eval = TRUE, out.width = "62%", fig.align = 'center'}
knitr::include_graphics("images/randomforest.png")
```

```{r, eval = FALSE}
# Creating random forest model
library(randomForest)

rf_model <- randomForest(formula = risk ~ .,
                         data = loan_data)
```

]

---
# How do I do machine learning in R?

.pull-left3[

For advanced users packages such as `mlr` and `caret` can automate much of the the machine learning process.

```{r, echo = FALSE, eval = TRUE, out.width = "100%", fig.align = 'center'}
knitr::include_graphics("images/mlrcaret.png")
```

]

.pull-right3[

In the practical, we will go through the basic steps "by hand" so you can see the process:

1. Get data
2. Train models
3. Predict new data
4. Compare training to prediction accuracy

]


---
# Questions?
