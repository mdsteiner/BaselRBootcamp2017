---
title: "Practical: Machine Learning (Answers)"
author: "BaselRBootcamp 2017"
output: html_document
---

```{r, echo = FALSE, fig.align = 'center', fig.cap = "Source: https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer"}
knitr::include_graphics("images/machinelearningcartoon.png")
```


```{r, echo = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, fig.align = 'center')
```


### Slides

Here a link to the lecture slides for this session: LINK

### Overview

In this practical you'll conduct machine learning analyses on a dataset on heart disease. You will see how well many different machine learning models can predict new data. By the end of this practical you will know how to:

1. Create separate training and test data
2. Fit a model to data
3. Make predictions from a model
4. Compare models in how well they canpredict new data.

### Key Functions and packages

Here are the main functions and packages you'll be using. For more information about the specific models, click on the link in *Additional Details*.

| Algorithm| Function| Package | Additional Details |
|:------|:--------|:----|:----|
|     Regression|    `glm()`| Base R| https://bookdown.org/ndphillips/YaRrr/regression.html#the-linear-model|
|     Support Vector Machines|    `svm()`| `e1071` |https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html |
|     Decision Trees|    `rpart()`| `rpart` | https://statweb.stanford.edu/~lpekelis/talks/13_datafest_cart_talk.pdf|
| Random Forests | `randomForest()` | `randomForest` | http://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/|


```{r, echo = TRUE, eval = FALSE}
# Install all packages necessary for the practical
#  Only necessary if you don't already have them

install.packages(c("e1071", "rpart", "randomForest", "tidyverse", "yarrr", "FFTrees"))
```


### Examples

- The following examples will take you through all steps of the machine learning process, from creating training and test data, to fitting models, to making predictions. Follow along and try to see how piece of code works!

```{r, eval = FALSE, echo = TRUE}
# -----------------------------------------------
# Examples of hypothesis tests on the ChickWeight data
# ------------------------------------------------

# -----------------------
# Part A:
# Load libraries
# -----------------------

library(e1071)          # for svm()
library(randomForest)   # for randomForest()
library(rpart)          # for rpart()
library(yarrr)          # for pirateplot()
library(tidyverse)      # for datawrangling and ggplot2
library(FFTrees)        # for the heartdisease data

# -----------------------
# Part B:
# Create heart a dataset containing all data
# -----------------------

heart <- heartdisease   # Save a copy of the heartdisease data as heart

for(i in 1:ncol(heart)) {  # Convert character columns and diagnosis to factor
  
  if(class(heart[,i]) == "character" | names(heart)[i] == "diagnosis") {
    
    heart[,i] <- factor(heart[,i])}
  
}

# -----------------------
# Part C: Create separate training and test data 
#  heart_train, heart_test
# -----------------------

set.seed(100)   # To fix the randomization
train_samples <- sample(nrow(heart), size = 125)
test_samples <- setdiff(1:nrow(heart), train_samples)

heart_train <- heart[train_samples, ]
heart_test <- heart[test_samples, ]

# ------------------------------
# Part I: Create models from training data
# ------------------------------

glm_model <- glm(formula = chol ~ ., data = heart_train)
svm_model <- svm(formula = chol ~ ., data = heart_train)
rpart_model <- rpart(formula = chol ~ ., data = heart_train)
randomforest_model <- randomForest(formula = chol ~ ., data = heart_train)

# ------------------------------
# Part II: Calculate fits for each model for heart_train
#  predict(model, heart_train)
# ------------------------------

glm_fit <- predict(glm_model, heart_train)
svm_fit <- predict(svm_model, heart_train)
rpart_fit <- predict(rpart_model, heart_train)
randomforest_fit <- predict(randomforest_model, heart_train)

# Calculate fitting absolute errors and put in a dataframe

fit_mae_df <- data_frame(index = 1:nrow(heart_train),
                         glm = abs(glm_fit - heart_train$chol),
                         svm = abs(svm_fit - heart_train$chol),
                         rpart = abs(rpart_fit - heart_train$chol),
                         randomforest = abs(randomforest_fit - heart_train$chol))

# ------------------------------
# Part III: Plot fitting results
# ------------------------------

# Create a second reshaped dataframe for ggplot

fit_mae_df2 <- fit_mae_df %>% 
  gather(model, mae, -index)

# Create violin plot with ggplot

ggplot(fit_mae_df2, 
       aes(model, mae)) + 
       geom_boxplot(aes(fill = factor(model))) +
       labs(title = "Cholestorol Fitting Performance", 
            y = "Mean Absolute Error (MAE)")

# Or a pirateplot with yarrr
yarrr::pirateplot(data = as.data.frame(fit_mae_df[,2:5]), 
                  main = "Cholesterol Fitting Accuracy")

# ------------------------------
# Part IV: Calculate predictions for each model for heart_test
# predict(model, heart_test)
# ------------------------------

# Calculate predictions for each model for heart_test

glm_pred <- predict(glm_model, heart_test)
svm_pred <- predict(svm_model, heart_test)
rpart_pred <- predict(rpart_model, heart_test)
randomforest_pred <- predict(randomforest_model, heart_test)

# Calculate prediction ae (mean absolute error)

truth_test <- heart_test$chol

glm_pred_ae <- abs(glm_pred - truth_test)
svm_pred_ae <- abs(svm_pred - truth_test)
rpart_pred_ae <- abs(rpart_pred - truth_test)
randomforest_pred_ae <- abs(randomforest_pred - truth_test)

pred_mae_df <- data_frame(index = 1:nrow(heart_test),
                          glm = glm_pred_ae,
                          svm = svm_pred_ae,
                          rpart = rpart_pred_ae,
                          randomforest = randomforest_pred_ae)

# ------------------------------
# Part V: Plot prediction results
# ------------------------------

# Create a second reshaped dataframe for ggplot
pred_mae_df2 <- pred_mae_df %>% 
              gather(model, mae, -index)

# Create violin plot

ggplot(pred_mae_df2, 
       aes(model, mae)) + 
       geom_boxplot(aes(fill = factor(model))) +
       labs(title = "Cholestorol Prediction Performance", 
            y = "Mean Absolute Error (MAE)")


# Or a pirateplot
yarrr::pirateplot(data = as.data.frame(pred_mae_df[,2:5]), 
                  main = "Cholesterol Prediction Accuracy")

```


## Tasks

- Note, most of this practical will be copying and pasting code from the Examples and only making small changes.
- Try running pieces of the code line by line and understand what it's doing.

#### Gettting started

A. For this practical, we'll use the `heartdisease` dataframe from the `FFTrees` package, load the package with the `library()` function. We'll also use the `e1071`, `randomForest`, `rpart`, `ggplot2`, and `yarrr` pacakges. 

```{r, message = FALSE, echo = TRUE, eval = TRUE, warning = FALSE}
# Load libraries

library(e1071)          # for svm()
library(randomForest)   # for randomForest()
library(rpart)          # for rpart()
library(yarrr)          # for pirateplot()
library(tidyverse)      # for datawrangling and ggplot2
library(FFTrees)        # for the heartdisease data
```


B. Now save a copy of the data as `heart`

```{r, echo = TRUE, eval = TRUE}
heart <- heartdisease
```


C. Take a look at the data by running `head()` and `View()`

```{r, echo = TRUE, eval = FALSE}
head(heart)
View(heart)
```

#### Create training and test data

1. Following Part B and Part C, create separate dataframes `heart_train` for model training and `heart_test`

   - You can just copy parts B and C directly into your code, you don't need to change anything.

```{r, echo = TRUE}
# -----------------------
# Part B:
# Create heart
# -----------------------


# Save a copy of the ChickWeight data as chick
heart <- heartdisease  

# Some of the columns need to be converted to factors
for(i in 1:ncol(heart)) {
  
  if(class(heart[,i]) == "character" | names(heart[i]) == "diagnosis") {
    
   heart[,i] <- factor(heart[,i])}
  
}

# -----------------------
# Part C: Create training and test data 
#  heart_train, heart_test
# -----------------------

# Determine which samples are for training and which are for testing

set.seed(100)   # To fix the randomization
train_samples <- sample(nrow(heart), size = 125)
test_samples <- setdiff(1:nrow(heart), train_samples)

# Create training and test data
heart_train <- heart[train_samples, ]
heart_test <- heart[test_samples, ]
```


#### Train models on trestbps

2. Now let's do some model training! In our analyses, we will try to predict each patient's value of `trestbps` -- their resting blood pressure, as a function of all other variables in the data. Following Part I, create four new objects `glm_trestbps_train`, `svm_trestbps_train`, `rpart_trestbps_train`, and `randomforest_trestbps_train` predicting `trestbps`

   - All you need to do is change what is being predicted in the formula


```{r, echo = TRUE, eval = TRUE}
# ------------------------------
# Part I: Create models from training data
# ------------------------------

glm_model <- glm(formula = trestbps ~ ., data = heart_train)
svm_model <- svm(formula = trestbps ~ ., data = heart_train)
rpart_model <- rpart(formula = trestbps ~ ., data = heart_train)
randomforest_model <- randomForest(formula = trestbps ~ ., data = heart_train)
```

#### Calculate fits for training data

4. Following Part II, calculate fits for each model with `predict(model, heart_train)`, then create `fit_mae_df` containing the absolute fitting errors for each model

   - The code will be almost identical to what is in the Example. All you need to do is change the value of `truth_train` to the correct column in `heart_train`

```{r}
# ------------------------------
# Part II: Calculate fits for each model for heart_train
#  predict(model, heart_train)
# ------------------------------

glm_fit <- predict(glm_model, heart_train)
svm_fit <- predict(svm_model, heart_train)
rpart_fit <- predict(rpart_model, heart_train)
randomforest_fit <- predict(randomforest_model, heart_train)

# Calculate fitting absolute errors and put in a dataframe

truth_train <- heart_train$trestbps

fit_mae_df <- data_frame(index = 1:nrow(heart_train),
                         glm = abs(glm_fit - truth_train),
                         svm = abs(svm_fit - truth_train),
                         rpart = abs(rpart_fit - truth_train),
                         randomforest = abs(randomforest_fit - truth_train))
```

5. Following Part III, plot the training results

   - Don't forget to change the labels to reflect that we are predicting trestbps, not cholesterol!


```{r}
# ------------------------------
# Part III: Plot Training results
# ------------------------------

# Create a second reshaped dataframe for ggplot

fit_mae_df2 <- fit_mae_df %>% 
  gather(model, mae, -index)

# Create violin plot with ggplot

ggplot(fit_mae_df2, 
       aes(model, mae)) + 
       geom_boxplot(aes(fill = factor(model))) +
       labs(title = "Trestbps Fitting Performance", 
            y = "Mean Absolute Error (MAE)")

# Or a pirateplot with yarrr
yarrr::pirateplot(data = as.data.frame(fit_mae_df[,2:5]), 
                  main = "Trestbps Fitting Accuracy, ")
```

#### Calculate predictions for test data


6. Following Part IV, calculate predictions for each model based on `heart_test`

   - Don't forget to change the value of `truth_test` to reflect `trestbps`!

```{r}
# ------------------------------
# Part IV: Calculate predictions for each model for heart_test
# predict(model, heart_test)
# ------------------------------

# Calculate predictions for each model for heart_test

glm_pred <- predict(glm_model, heart_test)
svm_pred <- predict(svm_model, heart_test)
rpart_pred <- predict(rpart_model, heart_test)
randomforest_pred <- predict(randomforest_model, heart_test)

# Calculate prediction ae (mean absolute error)

truth_test <- heart_test$trestbps

glm_pred_ae <- abs(glm_pred - truth_test)
svm_pred_ae <- abs(svm_pred - truth_test)
rpart_pred_ae <- abs(rpart_pred - truth_test)
randomforest_pred_ae <- abs(randomforest_pred - truth_test)

pred_mae_df <- data_frame(index = 1:nrow(heart_test),
                          glm = glm_pred_ae,
                          svm = svm_pred_ae,
                          rpart = rpart_pred_ae,
                          randomforest = randomforest_pred_ae)
```

7. Following part V, plot the prediction results! Which model was the best in predicting resting heart rate in the test set?

   - Don't forget to change the names of the labels to reflect that we're predicting trestbps!

```{r}
# ------------------------------
# Part V: Plot prediction results
# ------------------------------

# Create a second reshaped dataframe for ggplot
pred_mae_df2 <- pred_mae_df %>% 
              gather(model, mae, -index)

# Create violin plot

ggplot(pred_mae_df2, 
       aes(model, mae)) + 
       geom_boxplot(aes(fill = factor(model))) +
       labs(title = "Resting blood pressure Prediction Performance", 
            y = "Mean Absolute Error (MAE)")


# Or a pirateplot
yarrr::pirateplot(data = as.data.frame(pred_mae_df[,2:5]), 
                  main = "Resting blood pressure Prediction Accuracy")
```


## Extras and Challenges

8. Of all the models we've used, the only one you can easily visualize is a decision tree created from `rpart`. Try visualizing the trees you created using the following code:

   - What does the rpart model tell you about the data?

```{r, echo = TRUE}
# Visualize the rpart model

plot(rpart_model)  # Plot the structure of the tree
text(rpart_model)  # Add labels
```

9. In the linear regression example `glm_model`, you can see which variables are significant by running `summary(glm_model)`. Try running this code to see which variables are significantly correlated with the criterion.

```{r}
summary(glm_model)
```


10. The randomforest model is difficult to visualize, but you can see which variables are deemed to be important in the dataset by accessing the result `importance()`. Try visualising the importance of the variables as follows:

```{r, echo = TRUE}
# Create a dataframe of theh importance results
importance_results <- as.data.frame(importance(randomforest_model))
importance_results$feature <- rownames(importance_results)

# Plot the result as a barplot
ggplot(importance_results, aes(feature, IncNodePurity, fill = feature)) + 
  geom_col()
```


11. Try repeating the same process as above (for either cholesterol or resting heart rate), but instead of training the data on 125 cases, try training it on only 50 cases.

   - How do you think having fewer cases in the training data will affect accuracy in fitting and prediction? 
   
12. So far we have only predicted continuous variables (`chol` and `trestbps`). However, all the machine learning models we have covered can also predict categorical values -- like whether patient's have a diagnosis value of 0 or 1. Try replicating your previous analyses, but now predict `diagnosis`.


   - In order to tell R that diagnosis is a factor (that is, a categorical variable), you need to convert it to a factor when running `formula()` each time you fit a model. For example, to run the support vector machines model, you'd run `svm_model <- svm(formula = factor(diagnosis) ~., data = heart_train)`. For the `glm()` model, you will additionally need to include the argument `family = 'binomial'` to tell the algorithm to run a logistic regression.

```{r, eval = FALSE}
# To model diagnosis, we can run the following
#  Note: we need to convert diagnosis to a factor with factor(diagnosis)

glm_model <- glm(formula = factor(diagnosis) ~ ., data = heart_train, family = "binomial")
svm_model <- svm(formula = factor(diagnosis) ~ ., data = heart_train)
rpart_model <- rpart(formula = factor(diagnosis) ~ ., data = heart_train)
randomforest_model <- randomForest(formula = factor(diagnosis) ~ ., data = heart_train)
```


   - Once you have fitted the models, you will need to change the fitting and prediction accuracy code in parts II and IV. Specifically, you can measure model error as the *disagreement* between model predictions and the true values. 
   
```{r, echo = TRUE, eval = FALSE}
# When modelling categorical values, then calculate model DISAGREEMENT 
#  as the percentage of cases where the model predictions 
#  are different from the true values

glm_pred_d <- mean(glm_pred != truth_test)     # Percent of cases where glm model is incorrect
svm_pred_d <- mean(svm_pred != truth_test)     # Percent of cases where svm model is incorrect 
rpart_pred_d <- mean(rpart_pred != truth_test) # [...]
randomforest_pred_d <- mean(randomforest_pred != truth_test) # [...]

# Combine model prediction errors just like in the Example

pred_d_df <- data_frame(index = 1:nrow(heart_test),
                          glm = glm_pred_d,
                          svm = svm_pred_d,
                          rpart = rpart_pred_d,
                          randomforest = randomforest_pred_d)
```



# Additional reading

- For more advanced machine learning functionality in R, check out the `caret` package [caret documentation link](http://topepo.github.io/caret/index.html) and the `mlr` package [mlr documentation link](https://cran.r-project.org/web/packages/mlr/vignettes/mlr.html)

- To read more about the fundamentals of machine learning and statistical prediction, check out [An Introduction to Statistical Learning by James et al.](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370)
